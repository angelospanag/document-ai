# 📄 document-ai

This is a simple FastAPI application that allows users to:

- ✅ Upload **PDF** or **DOCX** documents
- 🧠 Get a **summary** generated by a local **LLM** (via [Ollama](https://ollama.com/))
- ❓ Ask **questions** about the content of uploaded documents

The app is fully local — no API keys or cloud model usage required.

<!-- TOC -->
* [📄 document-ai](#-document-ai)
  * [⚡ Features](#-features)
  * [🚀 Quick Start](#-quick-start)
    * [1. Install Python 3, uv and ollama](#1-install-python-3-uv-and-ollama)
    * [2. Create a virtual environment with all necessary dependencies](#2-create-a-virtual-environment-with-all-necessary-dependencies)
    * [3. Create a `.env` file at the root of the project](#3-create-a-env-file-at-the-root-of-the-project)
  * [Run application](#run-application)
    * [Development mode](#development-mode)
    * [Production mode](#production-mode)
    * [3. Run ollama locally using `llama3.2`](#3-run-ollama-locally-using-llama32)
  * [🔌 API Endpoints](#-api-endpoints)
    * [📄 `POST /upload/` — **Summarize a document**](#-post-upload--summarize-a-document)
      * [✅ Request](#-request)
      * [🤖 Response](#-response)
    * [❓ `POST /ask/` — **Ask a question about a document**](#-post-ask--ask-a-question-about-a-document)
      * [🤖 Response](#-response-1)
<!-- TOC -->

## ⚡ Features

- 🔍 **Summarization** of uploaded documents using LLMs (like LLaMA3, Mistral, etc.)
- 🤖 **Context-aware Q&A** on document content
- 🛡️ Type-safe response models using `pydantic`
- 📂 Supports `.pdf` and `.docx` files
- 🔧 Easily swappable LLM backend (via Ollama)

---

## 🚀 Quick Start

### 1. Install Python 3, uv and ollama

**MacOS (using `brew`)**

```bash
brew install python@3.13 uv ollama
```

**Ubuntu/Debian**

```bash
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.13
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Create a virtual environment with all necessary dependencies

From the root of the project execute:

```bash
uv sync
```

### 3. Create a `.env` file at the root of the project

```dotenv
# Model name running locally on Ollama
MODEL_NAME=llama3.2
```

## Run application

### Development mode

```bash
uv run fastapi dev app/main.py
```

### Production mode

```bash
uv run fastapi run app/main.py
```

### 3. Run ollama locally using `llama3.2`

```bash
ollama run llama3.2
```

## 🔌 API Endpoints

### 📄 `POST /upload/` — **Summarize a document**

Uploads a PDF or DOCX file and returns a summarized version of its contents using a local LLM via Ollama.

#### ✅ Request

- **Method:** `POST`
- **URL:** `/upload/`
- **Content-Type:** `multipart/form-data`
- **Form Fields:**
    - `file`: PDF or DOCX file

#### 🤖 Response

```json
{
  "summary": "This document is about..."
}
```

### ❓ `POST /ask/` — **Ask a question about a document**

Uploads a PDF or DOCX file along with a natural language question. The local LLM will generate an answer based on the
file’s contents.

✅ Request

- **Method:** `POST`
- **URL:** `/ask/`
- **Content-Type:** `multipart/form-data`
- **Form Fields:**
    - `file`: PDF or DOCX file
    - `question`: Your question as plain text

#### 🤖 Response

```bash
{
  "answer": "The document describes..."
}
```

🧪 Example cURL

```bash
curl -X POST http://localhost:8000/ask/ \
  -F "file=@your_file.docx" \
  -F "question=What is the main idea of this document?"
```
