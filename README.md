# 📄 document-ai

This is a simple FastAPI application that allows users to:

- ✅ Upload **PDF** or **DOCX** documents
- 🧠 Get a **summary** generated by a local **LLM** (via [Ollama](https://ollama.com/))
- ❓ Ask **questions** about the content of uploaded documents

The app is fully local — no API keys or cloud model usage required.

<!-- TOC -->
* [📄 document-ai](#-document-ai)
  * [⚡ Features](#-features)
  * [🚀 Quick Start](#-quick-start)
    * [1. Install Python 3, uv and ollama](#1-install-python-3-uv-and-ollama)
    * [2. Create a virtual environment with all necessary dependencies](#2-create-a-virtual-environment-with-all-necessary-dependencies)
    * [3. Create a `.env` file at the root of the project](#3-create-a-env-file-at-the-root-of-the-project)
    * [4. Run `llama3.2` locally using Ollama)](#4-run-llama32-locally-using-ollama)
    * [5. Run PostgreSQL and perform migrations`](#5-run-postgresql-and-perform-migrations)
  * [Run application](#run-application)
    * [Development mode](#development-mode)
    * [Production mode](#production-mode)
  * [Linting](#linting)
  * [Formatting](#formatting)
<!-- TOC -->

## ⚡ Features

- 🔍 **Summarization** of uploaded documents using LLMs (like LLaMA3, Mistral, etc.)
- 🤖 **Context-aware Q&A** on document content
- 🛡️ Type-safe response models using `pydantic`
- 📂 Supports `.pdf` and `.docx` files
- 🔧 Easily swappable LLM backend (via Ollama)

---

## 🚀 Quick Start

### 1. Install Python 3, uv and ollama

**MacOS (using `brew`)**

```bash
brew install python@3.13 uv ollama
```

**Ubuntu/Debian**

```bash
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.13
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Create a virtual environment with all necessary dependencies

From the root of the project execute:

```bash
uv sync
```

### 3. Create a `.env` file at the root of the project

```dotenv
# Ollama
MODEL_NAME=llama3.2

# Database
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_NAME=postgres
DATABASE_HOST=localhost
DATABASE_PORT=5432
```

### 4. Run `llama3.2` locally using [Ollama](https://ollama.com/))

```bash
ollama run llama3.2
```

### 5. Run PostgreSQL and perform migrations`

```bash
docker compose up -d
alembic upgrade head
```

## Run application

### Development mode

```bash
uv run fastapi dev app/main.py
```

### Production mode

```bash
uv run fastapi run app/main.py
```

## Linting

```bash
ruff check app/* tests/*
```

## Formatting

```bash
ruff format app/* tests/*
```